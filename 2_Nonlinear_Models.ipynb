{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Nonlinear Models**\n",
    "---\n",
    "## **Summary**\n",
    "to be added ..\n",
    "    \n",
    "## **References**\n",
    "1. Goodfellow, I., Bengio, Y., Courville, A. 2016. Deep Learning. MIT Press.\n",
    "2. Geron, A. 2017. Hands-On Machine Learning with Scikit-Learn & TensorFlow. O'Reily.\n",
    "3. Albon, C. 2018. Python Machine Learning Cookbook. O'Reily.\n",
    "4. IBM Data Science Professional Certificate Program. 2018. \n",
    "5. MIT Data Science and Big Data Cerfiticate Program. 2019.\n",
    "\n",
    "## **Concept**\n",
    "### **Polynomial Regression**\n",
    "\n",
    "* single variable\n",
    "    * $y_i = \\alpha + \\beta_1 x^2 + \\dots + \\beta_i x^i + \\varepsilon_i$\n",
    "* multiple variable\n",
    "    * example (2 variables): $\\hat{y} = \\alpha + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2 + \\beta_4 X_1^2 + \\beta_5 X_2^2 + \\varepsilon$\n",
    "    \n",
    "#### Regularization\n",
    "* Reduce the degrees\n",
    "\n",
    "### **Logistic Regression**\n",
    "* probabilty estimation: $\\hat{p} = h_\\theta(x) = \\sigma(\\theta^T\\cdot{x})$\n",
    "* sigmoid function (s-shaped): $\\sigma(\\cdot)$\n",
    "* logistic function: $\\sigma(t) = \\frac{1}{1+e^{-t}}$\n",
    "* prediction: $ \\begin{equation}\n",
    "                   \\hat{y} =\\left\\{\n",
    "                   \\begin{array}{@{}ll@{}}\n",
    "                      0 & \\text{if } \\hat{p} < 0.5, \\\\\n",
    "                      1 & \\text{if } \\hat{p} \\geq{0.5}.\n",
    "                   \\end{array}\\right.\n",
    "                \\end{equation}$\n",
    "\n",
    "#### Cost function\n",
    "* single training instance: $\\begin{equation}\n",
    "                               c(\\theta) =\\left\\{\n",
    "                               \\begin{array}{@{}ll@{}}\n",
    "                                   -log(\\hat{p})      & \\text{if } y = 1, \\\\\n",
    "                                   -log(1 - \\hat{p}) & \\text{if } y = 0.\n",
    "                               \\end{array}\\right.\n",
    "                             \\end{equation}$\n",
    "\n",
    "* cost function of all instance: $J(\\theta) = -\\frac1{m}\\sum_{i=1}^m[y^i log(\\hat{p}^i)+(1-y^i)log(1-\\hat{p}^i)]$\n",
    "\n",
    "* **convex** $\\Longrightarrow$ Gradient Descent **GUARANTEES** the global minimum\n",
    "    * logistic cost function partial deriviatives: $\\frac{\\partial}{\\partial{\\theta}_j}J(\\theta)=\\frac1{m}\\sum_{i=1}^m(\\sigma(\\theta^T\\cdot{x^i})-y^i)x_j^i$\n",
    "\n",
    "### **Softmax Regression**\n",
    "* support multiple classes \n",
    "* no need to train and combine multiple binary classifiers\n",
    "\n",
    "#### Softmax Function\n",
    "* 1. compute a score $s_k(x)$ for each class k, given an instance x: $s_k(x)=\\theta_k^T\\cdot{x}$\n",
    "* 2. apply softmax function to the score to estimate the probablity of each class\n",
    "* 3. each class: own dedicated parameter vector $\\theta_k$ \n",
    "* 4. together: all $\\theta_k$ stored as rows in a parameter matrix $\\Theta$.\n",
    "* 5. finally: $\\hat{p}_k=\\sigma(s(x))_k=\\frac{exp(s_k(x))}{\\displaystyle\\sum_{j=1}^K exp(s_j(x))}$\\\n",
    "    **K**: the number of classes\\\n",
    "    **s(x)**: a vector containing the socres of each class for the instance x\\\n",
    "    __$\\sigma(s(x))$__: the estimated probablity that the instance x belongs to class k given the scores of each class for that instance\n",
    "\n",
    "#### Prediction\n",
    "$\\DeclareMathOperator*{\\argmax}{arg\\max}$\n",
    "$$\\hat{y} = \\argmax_k \\sigma(s(x))_k = \\argmax_k s_k(x)= \\argmax_k(\\theta_k^T\\cdot{x}) $$\n",
    "* Predicting the highest estimated probablity (simply the class with the highest score) <br>\n",
    "* argmax operator returns the value of a variable that maximizes a function\n",
    "* returns the value of k that maximizes the estiamted probablity $\\sigma(s(x))_k$.\n",
    "\n",
    "#### 4.3.3 Cross entropy –– minimize the cost function\n",
    "* To measure how well a set of estimated class probablities match the target clases\n",
    "\n",
    "* Cross entropy cost function\n",
    "$$J(\\Theta) = - \\frac{1}{m}\\sum_{i=1}^m\\sum_{k=1}^K y_k^i log(\\hat{p}_k^i)$$\n",
    "* $y_k^i$ = 1, if the targe class for the *i*th instance is _k_;\n",
    "* y = 0 otherwise\n",
    "* K = 2, equivalent to the Logistic Regression's cost function\n",
    "\n",
    "The cross entropy between two probability distributions p and q is defined as $H(p, q)= -\\sum_x p(x)\\log  q(x)$\n",
    "\n",
    "##### **The gradient vector of the cost function with regard to $\\theta_k$:\n",
    "$$ \\nabla\\theta_kJ(\\Theta) = \\frac{1}{m}\\sum_{i=1}^m(\\hat{p}_k^i - y_k^i)x^i$$\n",
    "* Compute gradient vector for each class, then use Gradient Descent (or other optimazaiton algorithm) to find the parameter matrix $\\Theta$ that minimize the cost function.\n",
    "\n",
    "### 5. Pipelines\n",
    "Data Pipelines simplify the steps of processing the data. We use the module  **Pipeline** to create a pipeline. We also use **StandardScaler** as a step in our pipeline.\n",
    "\n",
    "### 6. Measures for In-Sample Evaluation\n",
    "\n",
    "When evaluating our models, not only do we want to visualise the results, but we also want a quantitative measure to determine how accurate the model is.\n",
    "\n",
    "Two very important measures that are often used in Statistics to determine the accuracy of a model are:\n",
    "\n",
    "- **$R^2$ / R-squared**\n",
    "- **Mean Squared Error (MSE)**\n",
    "\n",
    "**R-squared**\n",
    "\n",
    "R squared, also known as the coefficient of determination, is a measure to indicate how close the data is to the fitted regression line.\n",
    "The value of the R-squared is the percentage of variation of the response variable (y) that is explained by a linear model.\n",
    "\n",
    "\n",
    "\n",
    "**Mean Squared Error (MSE)**\n",
    "\n",
    "The Mean Squared Error measures the average of the squares of errors, that is, the difference between actual value (y) and the estimated value (ŷ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
