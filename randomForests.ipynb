{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center><font size=6><b>Random Forests</b></font></h1>\n",
    "<hr>\n",
    "\n",
    "## **Summary**\n",
    "#### **Decision trees**\n",
    "\n",
    "The basis of tree-based learners is the decision tree wherein a series of decision rules (e.g., “If their gender is male...”) are chained. The result looks vaguely like an upside-down tree, with the first decision rule at the top and subsequent decision rules spreading out below. In a decision tree, every decision rule occurs at a decision node, with the rule creating branches leading to new nodes. A branch without a decision rule at the end is called a *leaf*. <font color=red><b>Note: the interpretability of decision tree is great.</b></font>\n",
    "\n",
    "`Sklearn.DecisionTreeClassifier` use Gini impurity:\n",
    "$$G(t) = 1 - \\sum_{i = 1}^{c}p_{i}^{2}$$\n",
    "where $G(t)$ is the Gini impurity at node $t$ and $p_i$ is the proportion of observations of the class $c$ at node $t$.\n",
    "\n",
    "<b>Decision tree regressor</b>, instead of reducing Gini impurity or entropy, potential splits are by default measured on how much reduce the MSE:\n",
    "$$\\text{MSE} = \\frac{1}{n}\\sum_{i = 1}^{n}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "#### **Random Forest ––– an ensamble method of the decision trees**\n",
    "\n",
    "* <font size=3, color=brown>Aggregating the predictions of a group of predictors(e.g., classifiers or regressors) $\\Longrightarrow$ better predictions than with the best individual predictor</font>\n",
    "* <font size=3>Using the Ensemble methods after attaining a few good predictors towards to the end of a project</font>\n",
    "* <font size=3>e.g., Netflix Prize Competition</font>\n",
    "* <font size=3>Ensemble methods: _bagging, boosting, stacking, etc._</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font size = 4.5>Table of Content</font></h2>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 10px\">\n",
    "<li><a href=\"#ref1\">1. Voting Classifier</a></li>\n",
    "<ul><div><li><a href=\"#ref1\">1.1 Training and Visualizing</a></div>\n",
    "<div><li><a href=\"#ref2\">1.2 Making Predictions</a></div>\n",
    "<div><li><a href=\"#ref3\">1.3 Model Intepretation</a></div>\n",
    "<div><li><a href=\"#ref4\">1.4 Estimating Class Probablities</a></div></ul>\n",
    "<li><a href=\"#ref5\">2. The CART Training Algorithm</a></li>\n",
    "<li><a href=\"#ref6\">3. Gini Impurity vs Entropy</a></li>\n",
    "<li><a href=\"#ref7\">4. Regularization Hyperparameters</a></li>\n",
    "<li><a href=\"#ref8\">5. Regression</a></li>\n",
    "<ul><div><li><a href=\"#ref9\">5.1 Classification and Regression Tree (CART) algothrim tries to minimize the MSE</a></li></div></ul>\n",
    "<li><a href=\"#ref10\">6. Instability</a></li>\n",
    "<li><a href=\"#ref11\">7. Interactive Visualization</a></li>\n",
    "<li><a href=\"#ref12\">8. Interesting References</a></li>\n",
    "    \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Voting Classifiers**\n",
    "#### **1.1 Definition**\n",
    "* <font size=3, color=brown>Having multiple classifier: e.g., Logistic Regression, SVM, Random Forest, k-Nearest Neighbors, and other classifiers</font>\n",
    "<img src=\"fig7_1.png\" width=500>\n",
    "\n",
    "* <font size=3>Majority-vote classifier (<font color=red>getting the most votes</font>) $\\Longrightarrow$ ***Hard Voting*** classifier</font>\n",
    "* <font size=3>If all classifiers can estimate class probabilities, SciKit-Learn  can predict the class with the highest class probability, averaged over all the individual classifier$\\Longrightarrow$ ***Soft Voting*** classifier</font>\n",
    "<img src=\"fig7_2.png\" width=500>\n",
    "\n",
    "* <font size=3, color=green>The more independent predictors are, the better the ensemle methods work $\\Longrightarrow$ Using different algorithms</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.2 Example using the moons dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr',\n",
       "                              LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='auto',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=None,\n",
       "                                                 solver='lbfgs', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False)),\n",
       "                             ('rf',\n",
       "                              RandomForestClassifier(bootstrap=True,\n",
       "                                                     ccp_alpha=0.0,\n",
       "                                                     class_weight=None,\n",
       "                                                     cr...\n",
       "                                                     oob_score=False,\n",
       "                                                     random_state=None,\n",
       "                                                     verbose=0,\n",
       "                                                     warm_start=False)),\n",
       "                             ('svc',\n",
       "                              SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                                  class_weight=None, coef0=0.0,\n",
       "                                  decision_function_shape='ovr', degree=3,\n",
       "                                  gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                                  probability=True, random_state=None,\n",
       "                                  shrinking=True, tol=0.001, verbose=False))],\n",
       "                 flatten_transform=True, n_jobs=None, voting='soft',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_moons = make_moons()\n",
    "X = df_moons[0]\n",
    "y = df_moons[1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state =58)\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf_hard = SVC()\n",
    "svm_clf_soft = SVC(probability=True)\n",
    "\n",
    "# Hard voting\n",
    "voting_clf_hard = VotingClassifier(estimators=[('lr', log_clf), \n",
    "                                               ('rf', rnd_clf), \n",
    "                                               ('svc', svm_clf_hard)],\n",
    "                                   voting = 'hard')\n",
    "\n",
    "voting_clf_hard.fit(X_train, y_train)\n",
    "# soft voting\n",
    "voting_clf_soft = VotingClassifier(estimators=[('lr', log_clf), \n",
    "                                               ('rf', rnd_clf), \n",
    "                                               ('svc', svm_clf_soft)],\n",
    "                                   voting = 'soft')\n",
    "\n",
    "voting_clf_soft.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Hard Voting:\n",
      "LogisticRegression 0.8\n",
      "RandomForestClassifier 0.85\n",
      "SVC 1.0\n",
      "VotingClassifier 0.85\n",
      "Results of Soft Voting:\n",
      "LogisticRegression 0.8\n",
      "RandomForestClassifier 0.85\n",
      "SVC 1.0\n",
      "VotingClassifier 0.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Results of Hard Voting:\")\n",
    "for clf in (log_clf, rnd_clf, svm_clf_hard, voting_clf_hard):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "    \n",
    "print(\"Results of Soft Voting:\")\n",
    "for clf in (log_clf, rnd_clf, svm_clf_soft, voting_clf_soft):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Bagging and Pasting** \n",
    "* <font size=3><b>Bootstrapping</b> the training data w/ and w/o replacement, respectively</font>\n",
    "* <font size=3>Train the model with random subsets of the training set with the same training algorithm</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision boundary plot function\n",
    "def plot_decision_boundary(clf, X, Y, cmap='Paired_r'):\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:,0].min() - 10*h, X[:,0].max() + 10*h\n",
    "    y_min, y_max = X[:,1].min() - 10*h, X[:,1].max() + 10*h\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.contourf(xx, yy, Z, cmap=cmap, alpha=0.25)\n",
    "    plt.contour(xx, yy, Z, colors='k', linewidths=0.7)\n",
    "    plt.scatter(X[:,0], X[:,1], c=Y, cmap=cmap, edgecolors='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.1 Example**\n",
    "* <font size=3>Training 500 classifiers with 100 instances randomly sampled from the training set with replacement</font>\n",
    "* <font size=3>n_jobs: the number of CPU cores to use (-1 indicate all cores)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), \n",
    "                            n_estimators = 500, \n",
    "                            max_samples = 80, \n",
    "                            bootstrap=True, \n",
    "                            n_jobs=-1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred =  bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.2 Out-of-Bag Evaluation**\n",
    "* <font size=3>BaggingClassifier samples _m_ training instances with replacement, where _m_ is the size of the training set</font>\n",
    "* <font size=3>k = _n- m_ (n: all instances): not sampled in the training model $\\Longrightarrow$ ***out-of-bag*** (oob) instances</font>\n",
    "* <font size=3>k instances can be used for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-Bag score: 0.95\n",
      "Accuracy socre: 1.0\n",
      "Out-of-Bag Decision Function: [[0.         1.        ]\n",
      " [0.00540541 0.99459459]\n",
      " [1.         0.        ]\n",
      " [0.91326531 0.08673469]\n",
      " [0.06508876 0.93491124]\n",
      " [0.         1.        ]\n",
      " [0.05945946 0.94054054]\n",
      " [0.9950495  0.0049505 ]\n",
      " [0.86813187 0.13186813]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.45294118 0.54705882]\n",
      " [0.0960452  0.9039548 ]\n",
      " [0.16939891 0.83060109]\n",
      " [1.         0.        ]\n",
      " [0.08823529 0.91176471]\n",
      " [0.1        0.9       ]\n",
      " [0.13714286 0.86285714]\n",
      " [0.87978142 0.12021858]\n",
      " [0.95604396 0.04395604]\n",
      " [0.         1.        ]\n",
      " [0.05617978 0.94382022]\n",
      " [0.22099448 0.77900552]\n",
      " [0.48913043 0.51086957]\n",
      " [0.9516129  0.0483871 ]\n",
      " [0.77245509 0.22754491]\n",
      " [0.87830688 0.12169312]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.92195122 0.07804878]\n",
      " [0.05660377 0.94339623]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.70621469 0.29378531]\n",
      " [0.05699482 0.94300518]\n",
      " [0.08571429 0.91428571]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.4010989  0.5989011 ]\n",
      " [0.65104167 0.34895833]\n",
      " [0.87845304 0.12154696]\n",
      " [0.98830409 0.01169591]\n",
      " [0.855      0.145     ]\n",
      " [0.         1.        ]\n",
      " [0.7245509  0.2754491 ]\n",
      " [0.89325843 0.10674157]\n",
      " [0.         1.        ]\n",
      " [0.         1.        ]\n",
      " [0.07253886 0.92746114]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.         1.        ]\n",
      " [0.         1.        ]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.39181287 0.60818713]\n",
      " [1.         0.        ]\n",
      " [0.90285714 0.09714286]\n",
      " [0.47340426 0.52659574]\n",
      " [1.         0.        ]\n",
      " [0.56937799 0.43062201]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.87096774 0.12903226]\n",
      " [0.83838384 0.16161616]\n",
      " [0.         1.        ]\n",
      " [0.92513369 0.07486631]\n",
      " [0.         1.        ]\n",
      " [0.00561798 0.99438202]\n",
      " [1.         0.        ]\n",
      " [0.98823529 0.01176471]\n",
      " [0.95238095 0.04761905]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [1.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators = 500, max_samples = 80, bootstrap=True, n_jobs=-1, oob_score = True)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "print('Out-of-Bag score:', bag_clf.oob_score_)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "print('Accuracy socre:', accuracy_score(y_test, y_pred))\n",
    "\n",
    "print('Out-of-Bag Decision Function:', bag_clf.oob_decision_function_[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Random Patches and Random Subspaces**\n",
    "* <font size=3>BaggingClassifier: can sampling features as well</font>\n",
    "* <font size=3>Sampling both instances and features $\\Longrightarrow$ <font color=brown>_Random Patches_ method</font> (max_features, bootstrap_features)</font>\n",
    "* <font size=3>Sampling only features $\\Longrightarrow$ <font color=orange>_Random Subspaces_ method</font> (boostrap=False, max_samples=1.0, bootstrap_features =True, max_features < 1.0)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Random Forests**\n",
    "* <font size=3>An ensemble of Decision Trees, generally trained via bagging method (sometimes pasting method)</font>\n",
    "* <font size=3 color=brown>_RandomForestClassifier_$\\Longrightarrow$ more convenient and optimized for Decision Trees</font>\n",
    "* <font size=3 color=brown>_RandomForestRegressor_</font>\n",
    "* <font size=3>_RandomForstClassifier_: ALL the hyperparameters of <font color=orange>_DecisionTreeClassifier_</font> + <font color=orange>_BaggingClassifier_</font></font>\n",
    "* <font size=3> Searching for the BEST feature AMONG random subsets of features, instead of the very best feature when splitting a node</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 500, max_leaf_nodes = 16, n_jobs = -1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rnd_clf.predict(X_test)\n",
    "\n",
    "# equal to bagging method as below\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(splitter='random', max_leaf_nodes=16),\n",
    "                           n_estimators = 500, max_samples = 1.0, bootstrap = True, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.1 Extremely Randomized Trees (Extra-Trees)**\n",
    "* <font size=3>Applying random thresholds for each features rather than searching for the best possible thresholds</font>\n",
    "* <font size=3 color=brown>Trading more bias for a lower variance</font>\n",
    "* <font size=3>Much faster than regular Random Forests</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2 Feature Importance**\n",
    "* <font size=3>More important features: close to the root; less important features: close to leaves</font>\n",
    "* <font size=3>Feature importance: computing the average depth across all trees in the forest</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.08627603608936044\n",
      "sepal width (cm) 0.02230614110986166\n",
      "petal length (cm) 0.4376025458314549\n",
      "petal width (cm) 0.45381527696932306\n"
     ]
    }
   ],
   "source": [
    "# Iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 500, n_jobs = -1)\n",
    "rnd_clf.fit(iris[\"data\"], iris['target'])\n",
    "for name, score in zip(iris['feature_names'], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mengchen/scikit_learn_data\n"
     ]
    }
   ],
   "source": [
    "# MINST dataset example\n",
    "\n",
    "# find the data location of the Scikit-Learn Data\n",
    "from sklearn.datasets.base import get_data_home \n",
    "print (get_data_home())\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "#mnist = fetch_mldata('MNIST original')\n",
    "#X, y = mnist['data'], mnist['target']\n",
    "\n",
    "#rnd_clf.fit(X, y)\n",
    "#feature_score = rnd_clf.feature_importances_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Boosting**\n",
    "* <font size=3><font color=orange>Any Ensemble method</font> combining several weak learners into a strong learner $\\Longrightarrow$ Training each classifier <font color=brown>sequentially</font>; each subsequent classifier tries to correct the predecessor</font>\n",
    "* <font size=3>_AdaBoost_(_Adaptive Boosting_) + _Gradient Boosting_</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5.1 AdoBoosting**\n",
    "* <font size=3>Focusing more and more on the hard cases</font>\n",
    "* <font size=3>Increasing the relative weight of misclassified training instances $\\Longrightarrow$ sequentially training the model with updated weights, so on</font>\n",
    "<img src=\"fig7_7.png\" width=500>\n",
    "\n",
    "* <font size=3.5, color=brown>AdaBoost Algorithm</font>\n",
    "\n",
    "  * <font size=3>Initially, $w^{(i)} = \\frac{1}{m}$, weighted error rate of the $j^{th}$ predictor</font> \n",
    "    $$r_{j} = \\frac{\\displaystyle\\sum_{\\substack{i=1 \\\\\n",
    "                               \\hat{y}_{j}^{(i)}\\neq{y^{(i)}}\n",
    "                               }}^{m}w^{(i)}}\n",
    "                               {\\displaystyle\\sum_{i=1}^{m}w^{(i)}}$$\n",
    "    <font size=3>where $\\hat{y}_{j}^{(i)}$ is the $j^{th}$ predictor's prediction fro the $i^{th}$ instance.</font>\n",
    "  * <font size=3>Predictor weight\n",
    "    $$\\alpha_{j} = \\eta log\\frac{1-r_j}{r_j}$$\n",
    "    <font size=3>the $\\alpha\\uparrow$, the more accurate the predictor is.</font>\n",
    "  * <font size=3>Boosting the misclassified instances with updated weights</font>\n",
    "    for $i = 1,2,..., m$\n",
    "    $$w^{(i)}\\leftarrow\n",
    "        \\begin{cases}\n",
    "            w^{(i)}                 & \\text{if }\\hat{y}_{j}^{(i)} = y^{(i)} \\\\\n",
    "            w^{(i)}exp\\big(\\alpha_{j}\\big)  & \\text{if }\\hat{y}_{j}^{(i)}\\neq{y^{(i)}}\n",
    "        \\end{cases}\n",
    "    $$\n",
    "    <font size=3>then all the instance weights are normalized (i.e., divided by $\\sum_{i=1}^{m}w^{(i)}$).</font>\n",
    "  * <font size=3>Stops until the desired number of predictors is reached, or a perfect predictor is found</font>\n",
    "  * <font size=3>AdaBoost Prediction: computing the predictors and weighs them using the predictor weights $\\alpha_{j}$.\n",
    "    $$\\hat{y}(x) = \\argmax_c\n",
    "                   \\displaystyle\\sum_{\\substack{j=1 \\\\\n",
    "                               \\hat{y}_{j}(x)= k\n",
    "                               }}^{N}\\alpha_{j}\n",
    "    $$\n",
    "    <font size=3>where $N$ is the number of predictors<font>\n",
    "  * <font size=3 color=brown>Scikit-Learn use a multiclass version of AdaBoost called _SAMME_ (Stagwise Additive Modeling using a Multiclass Exponential loss function)</font>\n",
    "            _SAMME.R_ (R stands for 'Real') for estimating probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "          learning_rate=0.5, n_estimators=200, random_state=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth = 1),  # max_depth --> Decision Stump is a Decision Tree with max_depth = 1: A tree composed of \n",
    "                             n_estimators = 200,                     # a single decision node plus two leaf nodes\n",
    "                             algorithm = 'SAMME.R',                  # If AdaBoost ensemble is overfitting, try reducing the number of estimators or more strongly\n",
    "                             learning_rate = 0.5)                    # regularizing the base estimator\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5.2 Gradient Boosting**\n",
    "* <font size=3>Fitting the new predictor to the <font color=red>_residual errors_</font> made by the predeccessor</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7320329d2ad7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtree_reg3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtree_reg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_reg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_reg3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# ALTERNATIVE Approach\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-7320329d2ad7>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtree_reg3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtree_reg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_reg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree_reg3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# ALTERNATIVE Approach\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_new' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X, y)\n",
    "\n",
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X, y2)\n",
    "\n",
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X, y3)\n",
    "\n",
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
    "\n",
    "# ALTERNATIVE Approach\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth =  2, n_estimators = 3, learning_rate = 1.0)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='fig7_9.png' width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=orange>***5.2.1 Shrinkage***</font>: setting the learning_rate hyperparameter low to generate more trees for better predictions\n",
    "<img src='fig7_10.png' width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=orange>***5.2.2 Optimal Number of Trees***</font>\n",
    "* <font size=3>using <font color=brown>*staged_predict()*</font> method, measuring the validation error at each stage of training to find the optimal number of trees, and finally train another GBRT ensemble using the optimal number of trees</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GradientBoostingRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6dfe7d0708e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mgbrt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientBoostingRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mgbrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GradientBoostingRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squred_error(y_val, y_pred)\n",
    "          for y_pred in gbrt.predict(X_val)]\n",
    "bst_n_estimators =  np.argmin(errors)\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font size=3>Early Stopping Implementation: setting <font color=green>*warm_start</font>=True*, keeping exsiting trees when the <font color=green>_fit()_</font> method is called</font>\n",
    "* <font size=3 color=brown>Stochastic Gradient Boosting</font>\n",
    "    * <font size=3>hyperparameter <font color=green>*subsample*</font>: specifies the fraction of training instances</font>\n",
    "    * <font size=3>Trading a high bias for a lower variance</font>\n",
    "    * <font size=3>Speeding up training</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth = 2, warm_start = True)\n",
    "\n",
    "min_val_error = float('inf')\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators =  n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5: # if when the validation error does not improve for five iterations in a row\n",
    "            break # early stopping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5.3 Stacking (staked generalization)**\n",
    "* <font size=3>Train a model to perform the aggregation of the predicitons of all predictors</font>\n",
    "<img src='fig7_12.png' width=600>\n",
    "* <font size=3>**Training steps** (using a <font color=red>*hold-out set*</font>)</font> \n",
    "  * <font size=3>Split the dataset to two: the first subset for traning the predicitons in the first layer</font>\n",
    "  * <font size=3>The first layer predictors are used to make predictions on the second (held-out) set</font>\n",
    "  * <font size=3>This generates three predicted values for each instance, forming a new input of three features</font>\n",
    "  * <font size=3>The blender is trained on the new input to predict the target value given the first layer's predictions\n",
    "    <img src='fig7_13.png' width =400><img src='fig7_14.png' width =400>\n",
    "  * <font size=3 color=brown>Multilayer Stacking Ensemble</font>\n",
    "    * <font size=3>1. Training several different blenders with different algorithms (e.g., Linear Regression, Random Forest Regression)</font>\n",
    "    * <font size=3>2. Split training set into three subsets</font>\n",
    "      * <font size=3>2.1 The first one train the first layer,</font>\n",
    "      * <font size=3>2.2 the second one uses predictions made by the predictors of the first layer</font>\n",
    "      * <font size=3>2.3 the third one uses predictions made by the predictors of the second layer</font>\n",
    "      * <font size=3>2.4 Making the prediction for a new instance by going through each layer sequentially</font>\n",
    "<img src='fig7_15.png' width=500>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
